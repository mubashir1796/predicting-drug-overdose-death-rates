{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Necessary Libraries\n",
        "The libraries are imported for data manipulation (pandas, numpy), plotting and visualization (matplotlib, seaborn), machine learning (sklearn), and performance evaluation (accuracy_score, classification_report)."
      ],
      "metadata": {
        "id": "1Tp-dU2xDzll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olIwina4GKpC"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for data processing, machine learning, and evaluation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data\n",
        "The dataset is loaded into the data variable using pandas.read_csv() for subsequent analysis and model training.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pR_kSegTD-Mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enhanced_dataset_path = '/content/Drug_overdose_death_rates_by_drug_typesexageraceand_Hispanic_origin_United_States.csv'\n",
        "enhanced_dataset = pd.read_csv(enhanced_dataset_path)\n",
        "\n",
        "print(\"First few rows of the enhanced dataset:\")\n",
        "print(enhanced_dataset.head())"
      ],
      "metadata": {
        "id": "WBw8rrFeGVNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for missing values and displaying the missing values\n"
      ],
      "metadata": {
        "id": "wIkpeY7VEv6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the dataset\n",
        "missing_values = enhanced_dataset.isnull().sum()\n",
        "\n",
        "# Display the count of missing values per column\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "KEfeHyfGKkSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling the missing values in the 'ESTIMATE' and 'Unemployment Rate' columns with their respective median values to ensure no missing data affects model training. The lines for 'drug_involved' and 'death_month' are commented out as those columns are not present in the dataset. Finally, the code rechecks and prints the missing values in the dataset after handling the missing data."
      ],
      "metadata": {
        "id": "pbjpVqQ0E7zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in the 'ESTIMATE' column with the median (important for target variable)\n",
        "enhanced_dataset['ESTIMATE'] = enhanced_dataset['ESTIMATE'].fillna(enhanced_dataset['ESTIMATE'].median())\n",
        "\n",
        "# Fill missing values in 'Unemployment Rate' with the median\n",
        "enhanced_dataset['Unemployment Rate'] = enhanced_dataset['Unemployment Rate'].fillna(enhanced_dataset['Unemployment Rate'].median())\n",
        "\n",
        "# The following columns were not found in the dataset, so the lines are commented out or removed.\n",
        "# enhased_dataset['drug_involved'] = enhanced_dataset['drug_involved'].fillna('Unknown')\n",
        "# enhanced_dataset['death_month'] = enhanced_dataset['death_month'].fillna(enhanced_dataset['death_month'].mode()[0])  # Fill with most frequent value\n",
        "\n",
        "# Recheck missing values after filling\n",
        "missing_values_after = enhanced_dataset.isnull().sum()\n",
        "print(\"\\nMissing values after handling:\")\n",
        "print(missing_values_after)"
      ],
      "metadata": {
        "id": "IpwdGGQ8Kxic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'Unemployment-to-Overdose Ratio' is created by dividing Unemployment Rate by Overdose Death Rate (target variable).\n",
        "\n",
        "Infinite values (e.g., from division by zero) in the new ratio are replaced with NaN and then filled with the median of the column.\n",
        "\n",
        "Finally, the enhanced dataset with the new feature is displayed, and missing values are rechecked to ensure proper data handling."
      ],
      "metadata": {
        "id": "PoSS4UjdGQO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 'Unemployment-to-Overdose Ratio'\n",
        "enhanced_dataset['Unemployment_to_Overdose_Ratio'] = enhanced_dataset['Unemployment Rate'] / enhanced_dataset['ESTIMATE']\n",
        "\n",
        "# The 'drug_involved' column was not found in the dataset, so the line is commented out.\n",
        "# enhanced_dataset['drug_involved_category'] = enhanced_dataset['drug_involved'].astype('category').cat.codes\n",
        "\n",
        "# Handle infinite values (e.g., from division by zero) by replacing them with NaN\n",
        "# And then fill NaN values with the median of the respective column\n",
        "import numpy as np\n",
        "for col in ['Unemployment_to_Overdose_Ratio']:\n",
        "    enhanced_dataset[col] = enhanced_dataset[col].replace([np.inf, -np.inf], np.nan)\n",
        "    enhanced_dataset[col] = enhanced_dataset[col].fillna(enhanced_dataset[col].median())\n",
        "\n",
        "# Check the enhanced dataset with new features\n",
        "print(\"\\nEnhanced Dataset with New Features:\")\n",
        "# Removed 'drug_involved_category' as the column was not found\n",
        "print(enhanced_dataset[['Unemployment_to_Overdose_Ratio']].head())\n",
        "\n",
        "# Recheck for any remaining missing values in these columns after handling inf/NaN\n",
        "print(\"\\nMissing values in ratio columns after handling inf/NaN:\")\n",
        "print(enhanced_dataset[['Unemployment_to_Overdose_Ratio']].isnull().sum())"
      ],
      "metadata": {
        "id": "rJRfVwezK97u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a scatter plot to visualize the relationship between Unemployment Rate and Overdose Death Rates in the dataset."
      ],
      "metadata": {
        "id": "8PT9JIqpGiYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot for Unemployment Rate vs Overdose Death Rates\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x=enhanced_dataset[\"Unemployment Rate\"], y=enhanced_dataset[\"ESTIMATE\"], color='blue')\n",
        "plt.title(\"Unemployment Rate vs Overdose Death Rates\")\n",
        "plt.xlabel(\"Unemployment Rate (%)\")\n",
        "plt.ylabel(\"Drug Overdose Death Rate (per 100,000 population)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cu6EgJpCLN4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a heatmap of the correlation matrix to visualize the relationships between different numerical features in the dataset."
      ],
      "metadata": {
        "id": "LlRsYPuBG1No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot heatmap of correlation matrix\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(enhanced_dataset.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Matrix of Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "udv0bECULYuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines the features (X) and target variable (y) for the machine learning model and then displays them."
      ],
      "metadata": {
        "id": "5wnBSlhTHAUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Features (X) and Target (y)\n",
        "X = enhanced_dataset.drop(columns=['ESTIMATE', 'YEAR', 'PANEL', 'STUB_NAME', 'STUB_LABEL', 'INDICATOR', 'UNIT', 'AGE'])  # Drop irrelevant and non-numeric columns and 'drug_involved'\n",
        "y = enhanced_dataset['ESTIMATE']  # Target variable (overdose death rate)\n",
        "\n",
        "# Display the feature set and target variable\n",
        "print(\"Features (X):\", X.columns)\n",
        "print(\"Target (y):\"), y.name"
      ],
      "metadata": {
        "id": "TEV6u9bELo_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code splits the dataset into training and testing sets, which are necessary steps in preparing data for machine learning model training and evaluation."
      ],
      "metadata": {
        "id": "ci1-BgXsHE6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shape of the resulting datasets\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')"
      ],
      "metadata": {
        "id": "KYadc1NsLy9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code aims to train and evaluate multiple ML models to predict drug overdose death rates based on various features, including the 'drug_involved' column, which is one-hot encoded to make it usable by the machine learning models."
      ],
      "metadata": {
        "id": "shQWIpg9HTfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
        "import pandas as pd # Import pandas for get_dummies\n",
        "\n",
        "# Before model training: One-hot encode the 'drug_involved' column in X_train and X_test\n",
        "X_train = pd.get_dummies(X_train, columns=['drug_involved'], drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, columns=['drug_involved'], drop_first=True)\n",
        "\n",
        "# Ensure that X_train and X_test have the same columns after one-hot encoding\n",
        "# This step is crucial if some categories are present in train but not in test, or vice-versa\n",
        "train_cols = X_train.columns\n",
        "test_cols = X_test.columns\n",
        "\n",
        "missing_in_test = set(train_cols) - set(test_cols)\n",
        "for c in missing_in_test:\n",
        "    X_test[c] = 0\n",
        "\n",
        "missing_in_train = set(test_cols) - set(train_cols)\n",
        "for c in missing_in_train:\n",
        "    X_train[c] = 0\n",
        "\n",
        "X_test = X_test[train_cols] # Ensure the order of columns is the same\n",
        "\n",
        "# Initialize the models\n",
        "models = {\n",
        "    'Decision Tree': DecisionTreeRegressor(),\n",
        "    'Random Forest': RandomForestRegressor(),\n",
        "    'KNN': KNeighborsRegressor(),\n",
        "    'Gradient Boosting': GradientBoostingRegressor()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "model_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)  # Fit the model to the training data\n",
        "    y_pred = model.predict(X_test)  # Predict on the test data\n",
        "\n",
        "    # Evaluate performance for regression models (MSE and R2)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    model_results[name] = {'MSE': mse, 'R2': r2}\n",
        "\n",
        "# Display the evaluation metrics for each model\n",
        "print(\"Model Performance Results:\")\n",
        "for model_name, metrics in model_results.items():\n",
        "    print(f\"{model_name}: MSE = {metrics['MSE']}, R² = {metrics['R2']}\")\n"
      ],
      "metadata": {
        "id": "7txXAwJkMArN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is designed to perform binary classification on the drug overdose death rates dataset to predict whether the overdose death rate is \"High\" or \"Low\". Several classification models are trained, evaluated, and compared based on performance metrics such as precision, recall, F1-score, and accuracy."
      ],
      "metadata": {
        "id": "I6i99KzcH7mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the dataset (assuming it's already loaded as enhanced_dataset)\n",
        "# dataset_path = '/content/Drug_overdose_death_rates_by_drug_typesexageraceand_Hispanic_origin_United_States.csv'\n",
        "# dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Define the target variable and features (using the already processed enhanced_dataset)\n",
        "X_classification = enhanced_dataset.drop(columns=['ESTIMATE', 'YEAR', 'PANEL', 'STUB_NAME', 'STUB_LABEL', 'INDICATOR', 'UNIT', 'AGE'])\n",
        "y_classification = enhanced_dataset['ESTIMATE']\n",
        "\n",
        "# Convert the target variable to binary classification ('High' vs 'Low' overdose death rate)\n",
        "# Example: 'High' if overdose rate > 20, else 'Low'\n",
        "y_class = np.where(y_classification > 20, \"High\", \"Low\")\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_classification, y_class, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encode the 'drug_involved' column in X_train_cls and X_test_cls\n",
        "X_train_cls = pd.get_dummies(X_train_cls, columns=['drug_involved'], drop_first=True)\n",
        "X_test_cls = pd.get_dummies(X_test_cls, columns=['drug_involved'], drop_first=True)\n",
        "\n",
        "# Ensure that X_train_cls and X_test_cls have the same columns after one-hot encoding\n",
        "# This step is crucial if some categories are present in train but not in test, or vice-versa\n",
        "train_cls_cols = X_train_cls.columns\n",
        "test_cls_cols = X_test_cls.columns\n",
        "\n",
        "missing_in_test_cls = set(train_cls_cols) - set(test_cls_cols)\n",
        "for c in missing_in_test_cls:\n",
        "    X_test_cls[c] = 0\n",
        "\n",
        "missing_in_train_cls = set(test_cls_cols) - set(train_cls_cols)\n",
        "for c in missing_in_train_cls:\n",
        "    X_train_cls[c] = 0\n",
        "\n",
        "X_test_cls = X_test_cls[train_cls_cols] # Ensure the order of columns is the same\n",
        "\n",
        "# Initialize the models\n",
        "models_cls = {\n",
        "    'Logistic Regression': LogisticRegression(solver='liblinear'), # Specify solver for LogisticRegression\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Store the results for each model\n",
        "model_results_cls = {}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models_cls.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred_cls = model.predict(X_test_cls)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    precision = precision_score(y_test_cls, y_pred_cls, average='binary', pos_label='High')\n",
        "    recall = recall_score(y_test_cls, y_pred_cls, average='binary', pos_label='High')\n",
        "    f1 = f1_score(y_test_cls, y_pred_cls, average='binary', pos_label='High')\n",
        "    accuracy = accuracy_score(y_test_cls, y_pred_cls)\n",
        "\n",
        "    # Store the results\n",
        "    model_results_cls[name] = {\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Accuracy': accuracy\n",
        "    }\n",
        "\n",
        "# Display the evaluation metrics for each model\n",
        "print(\"Model Performance Results (Classification):\")\n",
        "for model_name, metrics in model_results_cls.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Precision: {metrics['Precision']:.2f}\")\n",
        "    print(f\"  Recall: {metrics['Recall']:.2f}\")\n",
        "    print(f\"  F1-Score: {metrics['F1-Score']:.2f}\")\n",
        "    print(f\"  Accuracy: {metrics['Accuracy']:.2f}\")\n"
      ],
      "metadata": {
        "id": "hHEUYjdnPS3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs hyperparameter tuning for the RandomForestRegressor model using GridSearchCV. The goal is to find the best combination of hyperparameters for improving the model's performance."
      ],
      "metadata": {
        "id": "pNFumbrTIT76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 4]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display the best parameters found\n",
        "print(f\"Best Parameters for Random Forest: {grid_search.best_params_}\")"
      ],
      "metadata": {
        "id": "GCal2PjrNdCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a bar chart to compare the R² (R-squared) values of different models, visualizing how well each model fits the data based on this evaluation metric."
      ],
      "metadata": {
        "id": "17agYnmuId02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create lists for model names and corresponding R² values\n",
        "model_names = list(model_results.keys())\n",
        "r2_scores = [metrics['R2'] for metrics in model_results.values()]\n",
        "\n",
        "# Plot a bar chart of R² values for model comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(model_names, r2_scores, color='skyblue')\n",
        "plt.xlabel('R² Score')\n",
        "plt.title('Model Performance Comparison (R²)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MA_nrwsHN03h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating a set of 4 bar charts to compare the performance of different classification models across 4 different evaluation metrics: Precision, Recall, F1-Score, and"
      ],
      "metadata": {
        "id": "G6RRjtsgInca"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fe958e7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare data for plotting\n",
        "metrics_df = pd.DataFrame(model_results_cls).T\n",
        "metrics_df.index.name = 'Model'\n",
        "\n",
        "# Create subplots for each metric\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.flatten() # Flatten the 2x2 array of axes for easy iteration\n",
        "\n",
        "metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'Accuracy']\n",
        "\n",
        "# Ensure the palette has enough unique colors for all models\n",
        "colors = sns.color_palette('viridis', n_colors=len(metrics_df.index))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    # Assign x to hue and set legend=False to address FutureWarning\n",
        "    sns.barplot(x=metrics_df.index, y=metrics_df[metric], hue=metrics_df.index, ax=axes[i], palette=colors, legend=False)\n",
        "    axes[i].set_title(f'{metric} Comparison Across Models')\n",
        "    axes[i].set_ylabel(metric)\n",
        "    axes[i].set_xlabel('Model')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}